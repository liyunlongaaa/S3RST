Will run the code on one GPU.
| distributed init (rank 0): env://
git:
  sha: 428b9a7b1554dbcb565a2d20bde55ea4d1da4c92, status: has uncommited changes, branch: main

audioset_pretrain: False
batch_size_per_gpu: 16
clip_grad: 3.0
dist_url: env://
drop_path_rate: 0.1
epochs: 100
eval: False
freeze_last_layer: 1
global_crops_scale: 3
gpu: 0
imagenet_pretrain: False
input_fdim: 80
local_crops_number: 0
local_crops_scale: 2
local_rank: 0
lr: 0.2
max_frames: 300
min_lr: 5e-05
model_size: tiny224
model_type: vit_tiny
momentum_teacher: 0.996
musan_lmdb_path: /home/yoos/Downloads/musan_lmdb/data.lmdb
musan_path: /data/musan_split
n_last_blocks: 1
norm_last_layer: True
num_workers: 10
optimizer: adamw
out_dim: 65536
output_dir: ./output
patch_size: 16
rank: 0
saveckp_freq: 1
seed: 3407
teacher_temp: 0.04
train_list: list/voxceleb1_train_list
train_path: /data/voxceleb
use_bn_in_head: False
use_fp16: False
val_list: list/trials.txt
val_path: /data/voxceleb/voxceleb1
vox_lmdb_path: /home/yoos/Downloads/vox1_train_lmdb/data.lmdb
warmup_epochs: 10
warmup_teacher_temp: 0.04
warmup_teacher_temp_epochs: 0
weight_decay: 0.04
weight_decay_end: 0.4
world_size: 1
Data loaded: there are 148642 audios.
Student and Teacher are built: they are both vit_tiny network.
Found checkpoint at ./output/checkpoint.pth
=> loaded 'student' from checkpoint './output/checkpoint.pth' with msg <All keys matched successfully>
=> loaded 'teacher' from checkpoint './output/checkpoint.pth' with msg <All keys matched successfully>
=> loaded 'optimizer' from checkpoint: './output/checkpoint.pth'
=> key 'fp16_scaler' not found in checkpoint: './output/checkpoint.pth'
=> loaded 'dino_loss' from checkpoint './output/checkpoint.pth' with msg <All keys matched successfully>
Loss, optimizer and schedulers ready.
Starting DINO training !
Epoch: [7/100]  [   0/9290]  eta: 3:31:45  loss: 10.538657 (10.538657)  lr: 0.008750 (0.008750)  wd: 0.044335 (0.044335)  time: 1.367671  data: 0.447221  max mem: 2070
Epoch: [7/100]  [ 100/9290]  eta: 0:26:06  loss: 11.064476 (9.693723)  lr: 0.008762 (0.008757)  wd: 0.044347 (0.044342)  time: 0.166933  data: 0.000059  max mem: 2084
Epoch: [7/100]  [ 200/9290]  eta: 0:25:46  loss: 11.068937 (10.374862)  lr: 0.008776 (0.008764)  wd: 0.044360 (0.044348)  time: 0.160414  data: 0.000067  max mem: 2084
